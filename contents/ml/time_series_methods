时间序列分类（TSC）常用方法整理
===

* 时间域距离——DTW Dynamic time warping 动态时间规整：允许序列某个时刻的点和另一个序列多个连续时刻的点想对应（Time Warping 时间规整）
* 微分距离——查分法：将微分距离作为原始序列距离的补充，是最终距离计算函数的补充。（原序列和一阶查分序列相结合）
* 局部特征类——将时间序列划分成几个时间间隔，从每个间隔中提取特征（均值，标准差，斜率…)
* 局部特征类——shapelet 在序列中找到最具辨别性的子序列（可解释性强）与位置无关的最佳匹配子序列
* 模型类——每个类别构建一个生成模型，通过度量新序列与模型间的相似性确定类别。准确率较差，使用较少
* 集成类——将两种或多种算法组合成一个单独的分类器，准确率更高，训练时间长。（经典的如EE，把不同举例度量分类器做基分类器，一共11个）
* 深度学习|判别式——分类器直接学习原始输入时间序列，输出类别概率分布
* 深度学习|判别式|特征工程——人工设计特征，然后再把样本和特征作为分类器输入，思路：受到CV的影响，把时间序列数据转化成图片，直接Input到神经网络，
* 深度学习|判别式|端到端——特征提取和分类都用了同一个网络，具有通用性，不依赖特定领域知识。
*      Muti－Layer Perception ，MLP（多层感知器，不同层之间是全连接的）
*      Convolutional Neural Networks, CNN（卷积神经网络，本质是多层感知器，优化了层次结构：有卷积计算层，激励层，池化层和全连接层）
* 深度学习|生成模型|——在训练分类器之前，为时间序列找到一个好的表示方法。一般在训练分类器之前安排一个无监督的步骤，对数据进行建模。
* 深度学习|生成模型|自动编码器—— 输入一个序列，Encoder阶段节点数减少，Decoder输出阶段希望和输入一模一样。为让Encoder学到原序列的特征【达成降维目的】，用Encoder作为分类器输入
*      RNN 循环神经网络，隐藏层出现了反馈，某个时刻T在隐藏层出了要输出还需要为下一时刻T+1提供一个权重矩阵，天然有时间序列特性
*      DBN 深度置信网络（Deep Belief Nets）和CNN比较相似，主要对一维数据比较有效，目前使用的较少，是全连接的，并且有预训练过程。
*      SDAE 多层降噪自编码器 (SDAE)，不同于一般的自编码器，编码层比输入层要宽
* 深度学习|生成模型|ESN回声状态网络——是循环神经网络的一种，目的是通过神经网络对时间序列进行重建。特殊的是：中间层中神经节点及其关联是随机的，将输出作为特征，作为分类器的输入
